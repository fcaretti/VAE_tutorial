{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision import datasets, transforms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome\n",
    "### This notebook is for you only if you have never trained a NN, or done it with torch\n",
    "In the rest of the tutorial you will see how Variational Autoencoders compare to traditional Autoencoders. However, both these models are based on Neural Networks, so it's necessary to first have an understanding of these. Here I will assume that you roughly know how they work, but you have never trained one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you should know that the big goal is to optimize a set of parameters, distributed across different layers of neurons. The way to do this is Stochastic Gradient Descent, but it may be unclear how to optimize, let's say, the neurons in the first layer, from a loss function defined at the end. Or, better, it's easy to say how, but in practice it would be a pain to do it in practice. For this reason, we use Automatic Differentiation (also, it you're italian, you may have heard someone state \"derivare è bovino, integrare è divino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4.,  9., 16., 25.], grad_fn=<AddBackward0>)\n",
      "Gradient of the function w.r.t. x:None\n",
      "Gradient of the function w.r.t. x:tensor([ 4.,  6.,  8., 10.])\n"
     ]
    }
   ],
   "source": [
    "#define a function on a tensor\n",
    "#Notice: the function has to be a scalar function for torch to do it automatically!\n",
    "def f(x):\n",
    "    print(x**2 + 2*x + 1)\n",
    "    return torch.sum(x**2 + 2*x + 1)\n",
    "#define a torch tensor\n",
    "x = torch.tensor([1.,2.,3.,4.], requires_grad=True)\n",
    "#apply the function to the tensor\n",
    "y = f(x)\n",
    "#now, the gradient of y with respect to x is:\n",
    "print(f'Gradient of the function w.r.t. x:{x.grad}')\n",
    "#the gradient is None because we haven't computed it yet\n",
    "#to compute the gradient, we need to call the backward() method\n",
    "y.backward()\n",
    "#now, the gradient of y with respect to x is:\n",
    "print(f'Gradient of the function w.r.t. x:{x.grad}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 4., 9.], grad_fn=<AddBackward0>)\n",
      "Gradient of the function w.r.t. x:tensor([0., 2., 4., 6.])\n"
     ]
    }
   ],
   "source": [
    "#as you can imagine, this can work when we concatenate functions as well\n",
    "#let's define a function that is a composition of two functions\n",
    "def g(x):\n",
    "    return x-2\n",
    "\n",
    "#as before, let's define a tensor\n",
    "x = torch.tensor([1.,2.,3.,4.], requires_grad=True)\n",
    "#now weapply the composition of functions to the tensor\n",
    "y = f(g(x))\n",
    "\n",
    "y.backward()\n",
    "\n",
    "print(f'Gradient of the function w.r.t. x:{x.grad}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ok, now we can move to NNs\n",
    "Basically, we can see a NN as a concatenation of function that ultimately enters the Loss function (which is, coincidentally, a scalar one), for which we can then have the gradient w.r.t. all the parameters in the network. I'll be less pedantic from now on, as it gets only a matter of implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "#download the MNIST dataset\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "#load only part of the dataset: 10000 training images and 1000 test images\n",
    "train_dataset = torch.utils.data.Subset(train_dataset, range(10000))\n",
    "test_dataset = torch.utils.data.Subset(test_dataset, range(1000))\n",
    "\n",
    "#create a loader, an object that can iterate over the dataset\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=len(test_dataset), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 5)\n",
    "for i in range(5):\n",
    "    ax[i].imshow(train_dataset[i][0][0], cmap='gray')\n",
    "    ax[i].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/10 - Train Loss: 0.4434529694781941, Test Loss: 0.5139255523681641, Train Accuracy: 0.8824  Test Accuracy: 0.86\n",
      "Epoch 1/10 - Train Loss: 0.3511787938654043, Test Loss: 0.42111119627952576, Train Accuracy: 0.9028  Test Accuracy: 0.878\n",
      "Epoch 2/10 - Train Loss: 0.293855026696518, Test Loss: 0.36173945665359497, Train Accuracy: 0.9184  Test Accuracy: 0.887\n",
      "Epoch 3/10 - Train Loss: 0.26481009042187104, Test Loss: 0.33676403760910034, Train Accuracy: 0.9274  Test Accuracy: 0.898\n",
      "Epoch 4/10 - Train Loss: 0.27918673999560106, Test Loss: 0.35755741596221924, Train Accuracy: 0.9213  Test Accuracy: 0.889\n",
      "Epoch 5/10 - Train Loss: 0.24897335346337338, Test Loss: 0.33722174167633057, Train Accuracy: 0.9299  Test Accuracy: 0.904\n"
     ]
    }
   ],
   "source": [
    "#initialize a Net object\n",
    "net = Net(28*28, 100, 10)\n",
    "#initialize an optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.1)\n",
    "#define the loss function\n",
    "criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "n_epochs = 10\n",
    "#train on the dataset\n",
    "for epoch in range(n_epochs):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        #flatten the images\n",
    "        data = data.view(-1, 28*28)\n",
    "        #set the gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "        #forward pass\n",
    "        output = net(data)\n",
    "        #compute the loss\n",
    "        loss = criterion(output, target)\n",
    "        #backward pass\n",
    "        loss.backward()\n",
    "        #update the weights\n",
    "        optimizer.step()\n",
    "        #print some info\n",
    "        \n",
    "    # Evaluation phase\n",
    "    net.eval()  # Set the network to evaluation mode\n",
    "\n",
    "    # Calculate training loss over the entire training set\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    with torch.no_grad():  # No need to track gradients\n",
    "        for train_data, train_target in train_loader:\n",
    "            train_data = train_data.view(-1, 28*28)\n",
    "            train_output = net(train_data)\n",
    "            train_loss += criterion(train_output, train_target).item()\n",
    "            train_correct += torch.sum(torch.argmax(train_output, dim=1) == train_target).item()\n",
    "    train_loss /= len(train_loader)\n",
    "    train_accuracy = train_correct / len(train_dataset)\n",
    "\n",
    "    # Calculate test loss\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for test_data, test_target in test_loader:\n",
    "            test_data = test_data.view(-1, 28*28)\n",
    "            test_output = net(test_data)\n",
    "            test_loss += criterion(test_output, test_target).item()\n",
    "            correct += torch.sum(torch.argmax(test_output, dim=1) == test_target).item()\n",
    "    test_loss /= len(test_loader)\n",
    "    test_accuracy = correct / len(test_dataset)\n",
    "\n",
    "    # Print epoch information\n",
    "    print(f'Epoch {epoch}/{n_epochs} - Train Loss: {train_loss}, Test Loss: {test_loss}, Train Accuracy: {train_accuracy}  Test Accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
